{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import fastparquet\n",
    "import warnings\n",
    "import geopy\n",
    "from geopy.point import Point\n",
    "import time\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta, date\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in price data\n",
    "price_data = pd.read_parquet('C:/Users/mattg/Desktop/Hobbies/airbnb_reports/bucket_data/prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data.id = price_data.id.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miami_example = price_data[price_data['id'] == '52364652.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "miami_example['pull_time'] = pd.to_datetime(miami_example['pull_time'])\n",
    "print('done with datatype change')\n",
    "middle_time = time.time()\n",
    "miami_example['pull_time'] = miami_example['pull_time'].dt.date\n",
    "print('done with change to date')\n",
    "end_time = time.time()\n",
    "\n",
    "print('Start Time '+ str(start_time))\n",
    "print('Middle Time '+ str(middle_time-start_time))\n",
    "print('End Time '+ str(end_time-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miami_example_by_pull_time = miami_example.groupby(['id','pull_time'])['cleaning_fee','service_fee','total_price'].mean().reset_index()\n",
    "miami_example_by_pull_time_count = miami_example.groupby(['pull_time','check_in'])['cleaning_fee','service_fee','total_price'].count().reset_index()\n",
    "\n",
    "# Create traces\n",
    "miami_example_grouped_fig = go.Figure()\n",
    "miami_example_grouped_fig.add_trace(go.Scatter(x=miami_example_by_pull_time['pull_time'], y=miami_example_by_pull_time['cleaning_fee'],\n",
    "                    mode='lines',\n",
    "                    name='Cleaning Fee'))\n",
    "miami_example_grouped_fig.add_trace(go.Scatter(x=miami_example_by_pull_time['pull_time'], y=miami_example_by_pull_time['service_fee'],\n",
    "                    mode='lines',\n",
    "                    name='Service Fee'))\n",
    "miami_example_grouped_fig.add_trace(go.Scatter(x=miami_example_by_pull_time['pull_time'], y=miami_example_by_pull_time['total_price'],\n",
    "                    mode='lines', \n",
    "                    name='Total Price'))\n",
    "miami_example_grouped_fig.update_layout(title_text=\"Pricing Data by Date\", xaxis_title='Date', yaxis_title='Price ($)')\n",
    "filename = \"newsletter_features/price_data_line_fig_miami_example.png\"\n",
    "miami_example_grouped_fig.write_image(filename, engine='kaleido', width=2000, height=500)\n",
    "miami_example_grouped_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(miami_example_by_pull_time_count, x=\"pull_time\",nbins=50)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "price_data['pull_time'] = pd.to_datetime(price_data['pull_time'])\n",
    "print('done with datatype change')\n",
    "middle_time = time.time()\n",
    "price_data['pull_time'] = price_data['pull_time'].dt.date ## Do we actually need this step if we structure it like the occupancy data? (greater than/less than pull on dates)\n",
    "print('done with change to date')\n",
    "\n",
    "end_time = time.time()\n",
    "print('Start Time '+ str(start_time))\n",
    "print('Middle Time '+ str(middle_time-start_time))\n",
    "print('End Time '+ str(end_time-start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_subtract = 30\n",
    "last_day_from = pd.to_datetime('05/31/2022')\n",
    "date_30_days_ago = last_day_from - timedelta(days=days_to_subtract)\n",
    "pull_time_old = pd.to_datetime('04/20/2022')\n",
    "pull_time_current = pd.to_datetime('05/05/2022')\n",
    "\n",
    "price_data_last30 = price_data[(price_data['check_in'] > str(date_30_days_ago)) & (price_data['check_in'] <= str(last_day_from))]\n",
    "price_data_last30['check_in'] = pd.to_datetime(price_data_last30['check_in'])\n",
    "price_data_last30['day_of_week'] = price_data_last30['check_in'].dt.dayofweek\n",
    "print(price_data_last30.shape)\n",
    "price_data_previous_pull = price_data_last30[price_data_last30['pull_time'] == pull_time_old]\n",
    "print(price_data_previous_pull.shape)\n",
    "price_data_current_pull = price_data_last30[price_data_last30['pull_time'] == pull_time_current]\n",
    "print(price_data_current_pull.shape)\n",
    "price_data_previous_pull_avgs = price_data_previous_pull.groupby(['id','day_of_week']).mean().reset_index()\n",
    "print(price_data_previous_pull_avgs.shape)\n",
    "price_data_current_pull_avgs = price_data_current_pull.groupby(['id','day_of_week']).mean().reset_index()\n",
    "print(price_data_current_pull_avgs.shape)\n",
    "price_trend = price_data_previous_pull_avgs.merge(price_data_current_pull_avgs, how='inner', on='id')\n",
    "print(price_trend.shape)\n",
    "price_trend.to_csv('price_trend_miami_for_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data_last30['pull_time'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a delta from month of June price vs the month of May price for all properties to see increase vs decrease Month over Month\n",
    "# Then, merge each id with its zipcode, state, and city (if city is there) from the flat csv files used in the eda_summary (listings_w_zips.csv)\n",
    "# Then, groupby on zipcode with mean on delta from June-May to get average price increase vs decrease in that state/zip/city\n",
    "\"\"\"\n",
    "price_data_last30_avgs = price_data_last30.groupby(['id','pull_time']).mean().reset_index()\n",
    "price_data_last30_avgs.to_csv('price_data_last30_avgs.csv')\n",
    "data_w_listing_loc = pd.read_csv('post_mort_and_roi_calcs_june19.csv')\n",
    "data_w_listing_loc = data_w_listing_loc[['id','zipcode','City','Metro','CountyName','State']]\n",
    "price_data_last30_avgs.id = price_data_last30_avgs.id.astype('float64')\n",
    "price_data_last30_avgs_w_loc = price_data_last30_avgs.merge(data_w_listing_loc, how='inner', on='id')\n",
    "florida_price_trends = price_data_last30_avgs_w_loc[price_data_last30_avgs_w_loc['State'] == 'FL']\n",
    "florida_price_trends = florida_price_trends.groupby('pull_time').mean(['cleaning_fee','service_fee','total_price']).reset_index()\n",
    "\n",
    "#price_data_date = price_data.groupby(['check_in'])['cleaning_fee','service_fee','total_price'].apply(lambda row: np.sum(row)/len(row)).reset_index()\n",
    "\n",
    "# Create traces\n",
    "florida_price_trends_fig = go.Figure()\n",
    "florida_price_trends_fig.add_trace(go.Scatter(x=price_data_date['check_in'], y=price_data_date['cleaning_fee'],\n",
    "                    mode='lines',\n",
    "                    name='Cleaning Fee'))\n",
    "florida_price_trends_fig.add_trace(go.Scatter(x=price_data_date['check_in'], y=price_data_date['service_fee'],\n",
    "                    mode='lines',\n",
    "                    name='Service Fee'))\n",
    "florida_price_trends_fig.add_trace(go.Scatter(x=price_data_date['check_in'], y=price_data_date['total_price'],\n",
    "                    mode='lines', \n",
    "                    name='Total Price'))\n",
    "florida_price_trends_fig.update_layout(title_text=\"Pricing Data by Pull Time\", xaxis_title='Date', yaxis_title='Price ($)')\n",
    "filename = \"newsletter_features/florida_price_trends_fig_june19.png\"\n",
    "florida_price_trends_fig.write_image(filename, engine='kaleido', width=2000, height=500)\n",
    "florida_price_trends_fig.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_trend['cleaning_fee_delta'] = price_trend['cleaning_fee_y'] - price_trend['cleaning_fee_x']\n",
    "price_trend['service_fee_delta'] = price_trend['service_fee_y'] - price_trend['service_fee_x']\n",
    "price_trend['total_price_delta'] = price_trend['total_price_y'] - price_trend['total_price_x']\n",
    "price_trend['cleaning_fee_delta_pct'] = price_trend['cleaning_fee_delta']/price_trend['cleaning_fee_x']*100\n",
    "price_trend['service_fee_delta_pct'] = price_trend['service_fee_delta']/price_trend['service_fee_x']*100\n",
    "price_trend['total_price_delta_pct'] = price_trend['total_price_delta']/price_trend['total_price_x']*100\n",
    "price_trend['display_price_delta_pct'] = price_trend['total_price_delta_pct'] - price_trend['service_fee_delta_pct'] - price_trend['cleaning_fee_delta_pct']\n",
    "data_w_listing_loc = pd.read_csv('post_mort_and_roi_calcs_june19.csv')\n",
    "data_w_listing_loc = data_w_listing_loc[['id','zipcode','City','Metro','CountyName','State']]\n",
    "price_trend['id'] = price_trend['id'].astype('float64')\n",
    "price_trend_wloc = price_trend.merge(data_w_listing_loc, how='inner', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florida_price_trends = price_trend_wloc[price_trend_wloc['State'] == 'FL']\n",
    "florida_price_trends_zip = florida_price_trends.groupby('zipcode').mean(['cleaning_fee_delta_pct','service_fee_delta_pct','total_price_delta_pct']).reset_index()\n",
    "florida_price_trends_city = florida_price_trends.groupby('City').mean(['cleaning_fee_delta_pct','service_fee_delta_pct','total_price_delta_pct']).reset_index()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "zips = florida_price_trends_zip['zipcode']\n",
    "vals = [florida_price_trends_zip['display_price_delta_pct'], florida_price_trends_zip['cleaning_fee_delta_pct'], florida_price_trends_zip['service_fee_delta_pct'], florida_price_trends_zip['total_price_delta_pct']]\n",
    "font_color = ['rgb(40,40,40)'] +  [['rgb(255,0,0)' if v < 0 else 'rgb(0,125,0)' for v in vals[k]] for k in range(4)]\n",
    "\n",
    "table_trace = go.Table(\n",
    "                 columnwidth= [50]+[50]+[50]+[50]+[50],\n",
    "                 columnorder=[0, 1, 2, 3, 4],\n",
    "                 header = dict(height = 40,\n",
    "                               values = [['<b>Zip Code</b>'], ['<b>Display Price</b>'], ['<b>Cleaning Fee</b>'], ['<b>Service Fee</b>'],['<b>Total Price</b>']],\n",
    "                               line = dict(color='rgb(50,50,50)'),\n",
    "                               align = ['left']*5,\n",
    "                               font = dict(color=['rgb(45,45,45)']*4, size=14),\n",
    "                             \n",
    "                              ),\n",
    "                 cells = dict(values = [zips, vals[0], vals[1], vals[2], vals[3]],\n",
    "                              line = dict(color='#506784'),\n",
    "                              align = ['left']*5,\n",
    "                              \n",
    "                              font = dict(family=\"Arial\", size=14, color=font_color),\n",
    "                              format = [None, \",.2f\"],  #add % sign here\n",
    "                              height = 30,\n",
    "                              fill = dict(color='rgb(245,245,245)'))\n",
    "                             )\n",
    "                 \n",
    "\n",
    "layout = go.Layout(width=850, height=650, autosize=False, \n",
    "              title_text='Recent Pricing Trends in Miami (%) (M/M)',\n",
    "                   title_x=0.5, showlegend=False)\n",
    "florda_price_trend_fig = go.Figure(data=[table_trace], layout=layout)\n",
    "filename = \"newsletter_features/florida_price_trends_table_june19.png\"\n",
    "florda_price_trend_fig.write_image(filename, engine='kaleido', width=875, height=700)\n",
    "florda_price_trend_fig.show()\n",
    "\n",
    "# Look to do one price value and look and price change by day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new notebook called occupancy_eda.ipynb and copy all this code except switch out occupancy and change up logic and table (maybe do like month over month occupancy or 3 month outlook on occupancy)\n",
    "# (could be like occupancy rate and if its green it went up and if its red it went down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_looking_days = 28\n",
    "last_day_from = pd.to_datetime('05/31/2022')\n",
    "pull_time_current = pd.to_datetime('05/05/2022') # Would be set as date for \"today\" in the future\n",
    "fwd_looking_date_current = pull_time_current + timedelta(days=fwd_looking_days)\n",
    "pull_time_old = last_day_from - timedelta(days=7)\n",
    "fwd_looking_date_old = pull_time_old + timedelta(days=fwd_looking_days)\n",
    "\n",
    "price_data_last30_current = price_data[(price_data['check_in'] > str(pull_time_current)) & (price_data['check_in'] <= str(fwd_looking_date_current))]\n",
    "price_data_last30_old = price_data[(price_data['check_in'] > str(pull_time_old)) & (price_data['check_in'] <= str(fwd_looking_date_old))]\n",
    "print(price_data_last30_current.shape)\n",
    "print(price_data_last30_old.shape)\n",
    "price_data_last30_current['check_in'] = pd.to_datetime(price_data_last30_current['check_in'])\n",
    "price_data_last30_old['check_in'] = pd.to_datetime(price_data_last30_old['check_in'])\n",
    "price_data_last30_current['day_of_week'] = price_data_last30_current['check_in'].dt.dayofweek\n",
    "price_data_last30_old['day_of_week'] = price_data_last30_old['check_in'].dt.dayofweek\n",
    "price_data_previous_pull_avgs = price_data_last30_current.groupby(['id','day_of_week']).mean().reset_index()\n",
    "print(price_data_previous_pull_avgs.shape)\n",
    "price_data_current_pull_avgs = price_data_last30_old.groupby(['id','day_of_week']).mean().reset_index()\n",
    "print(price_data_current_pull_avgs.shape)\n",
    "price_trend = price_data_previous_pull_avgs.merge(price_data_current_pull_avgs, how='inner', on='id')\n",
    "print(price_trend.shape)\n",
    "price_trend.to_csv('price_trend_miami_for_table_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_trend['cleaning_fee_delta'] = price_trend['cleaning_fee_y'] - price_trend['cleaning_fee_x']\n",
    "price_trend['service_fee_delta'] = price_trend['service_fee_y'] - price_trend['service_fee_x']\n",
    "price_trend['total_price_delta'] = price_trend['total_price_y'] - price_trend['total_price_x']\n",
    "price_trend['cleaning_fee_delta_pct'] = price_trend['cleaning_fee_delta']/price_trend['cleaning_fee_x']*100\n",
    "price_trend['service_fee_delta_pct'] = price_trend['service_fee_delta']/price_trend['service_fee_x']*100\n",
    "price_trend['total_price_delta_pct'] = price_trend['total_price_delta']/price_trend['total_price_x']*100\n",
    "price_trend['display_price_delta_pct'] = price_trend['total_price_delta_pct'] - price_trend['service_fee_delta_pct'] - price_trend['cleaning_fee_delta_pct']\n",
    "data_w_listing_loc = pd.read_csv('post_mort_and_roi_calcs_june19.csv')\n",
    "data_w_listing_loc = data_w_listing_loc[['id','zipcode','City','Metro','CountyName','State']]\n",
    "price_trend['id'] = price_trend['id'].astype('float64')\n",
    "price_trend_wloc = price_trend.merge(data_w_listing_loc, how='inner', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florida_price_trends = price_trend_wloc[price_trend_wloc['State'] == 'FL']\n",
    "florida_price_trends_zip = florida_price_trends.groupby('zipcode').mean(['cleaning_fee_delta_pct','service_fee_delta_pct','total_price_delta_pct']).reset_index()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "zips = florida_price_trends_zip['zipcode']\n",
    "vals = [florida_price_trends_zip['display_price_delta_pct'], florida_price_trends_zip['cleaning_fee_delta_pct'], florida_price_trends_zip['service_fee_delta_pct'], florida_price_trends_zip['total_price_delta_pct']]\n",
    "font_color = ['rgb(40,40,40)'] +  [['rgb(255,0,0)' if v < 0 else 'rgb(0,125,0)' for v in vals[k]] for k in range(4)]\n",
    "\n",
    "table_trace = go.Table(\n",
    "                 columnwidth= [50]+[50]+[50]+[50]+[50],\n",
    "                 columnorder=[0, 1, 2, 3, 4],\n",
    "                 header = dict(height = 40,\n",
    "                               values = [['<b>Zip Code</b>'], ['<b>Display Price</b>'], ['<b>Cleaning Fee</b>'], ['<b>Service Fee</b>'],['<b>Total Price</b>']],\n",
    "                               line = dict(color='rgb(50,50,50)'),\n",
    "                               align = ['left']*5,\n",
    "                               font = dict(color=['rgb(45,45,45)']*4, size=14),\n",
    "                             \n",
    "                              ),\n",
    "                 cells = dict(values = [zips, vals[0], vals[1], vals[2], vals[3]],\n",
    "                              line = dict(color='#506784'),\n",
    "                              align = ['left']*5,\n",
    "                              \n",
    "                              font = dict(family=\"Arial\", size=14, color=font_color),\n",
    "                              format = [None, \",.2f\"],  #add % sign here\n",
    "                              height = 30,\n",
    "                              fill = dict(color='rgb(245,245,245)'))\n",
    "                             )\n",
    "                 \n",
    "\n",
    "layout = go.Layout(width=850, height=650, autosize=False, \n",
    "              title_text='Recent Pricing Trends in Miami (%) (W/W)',\n",
    "                   title_x=0.5, showlegend=False)\n",
    "florda_price_trend_fig = go.Figure(data=[table_trace], layout=layout)\n",
    "filename = \"newsletter_features/florida_price_trends_table_weekly.png\"\n",
    "florda_price_trend_fig.write_image(filename, engine='kaleido', width=875, height=700)\n",
    "florda_price_trend_fig.show()\n",
    "\n",
    "# Look to do one price value and look and price change by day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florida_price_trends = price_trend_wloc[price_trend_wloc['State'] == 'FL']\n",
    "florida_price_trends_city = florida_price_trends.groupby('City').mean(['cleaning_fee_delta_pct','service_fee_delta_pct','total_price_delta_pct']).reset_index()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "cities = florida_price_trends_city['City']\n",
    "vals = [florida_price_trends_city['display_price_delta_pct'], florida_price_trends_city['cleaning_fee_delta_pct'], florida_price_trends_city['service_fee_delta_pct'], florida_price_trends_city['total_price_delta_pct']]\n",
    "font_color = ['rgb(40,40,40)'] +  [['rgb(255,0,0)' if v < 0 else 'rgb(0,125,0)' for v in vals[k]] for k in range(4)]\n",
    "\n",
    "table_trace = go.Table(\n",
    "                 columnwidth= [50]+[50]+[50]+[50]+[50],\n",
    "                 columnorder=[0, 1, 2, 3, 4],\n",
    "                 header = dict(height = 40,\n",
    "                               values = [['<b>City</b>'], ['<b>Display Price</b>'], ['<b>Cleaning Fee</b>'], ['<b>Service Fee</b>'],['<b>Total Price</b>']],\n",
    "                               line = dict(color='rgb(50,50,50)'),\n",
    "                               align = ['left']*5,\n",
    "                               font = dict(color=['rgb(45,45,45)']*4, size=14),\n",
    "                             \n",
    "                              ),\n",
    "                 cells = dict(values = [cities, vals[0], vals[1], vals[2], vals[3]],\n",
    "                              line = dict(color='#506784'),\n",
    "                              align = ['left']*5,\n",
    "                              \n",
    "                              font = dict(family=\"Arial\", size=14, color=font_color),\n",
    "                              format = [None, \",.2f\"],  #add % sign here\n",
    "                              height = 30,\n",
    "                              fill = dict(color='rgb(245,245,245)'))\n",
    "                             )\n",
    "                 \n",
    "\n",
    "layout = go.Layout(width=850, height=650, autosize=False, \n",
    "              title_text='Recent Pricing Trends in Miami (%) (W/W)',\n",
    "                   title_x=0.5, showlegend=False)\n",
    "florda_price_trend_fig = go.Figure(data=[table_trace], layout=layout)\n",
    "filename = \"newsletter_features/florida_price_trends_table_cities_weekly.png\"\n",
    "florda_price_trend_fig.write_image(filename, engine='kaleido', width=875, height=700)\n",
    "florda_price_trend_fig.show()\n",
    "\n",
    "# Look to do one price value and look and price change by day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day of week for price comparison\n",
    "#price_trend = pd.read_csv('price_trend_miami_for_table.csv')\n",
    "\n",
    "# Calculate pull_time deltas for price\n",
    "price_trend['cleaning_fee_delta'] = price_trend['cleaning_fee_y'] - price_trend['cleaning_fee_x']\n",
    "price_trend['service_fee_delta'] = price_trend['service_fee_y'] - price_trend['service_fee_x']\n",
    "price_trend['total_price_delta'] = price_trend['total_price_y'] - price_trend['total_price_x']\n",
    "#price_trend['display_price_delta'] = price_trend['total_price_delta'] - price_trend['service_fee_delta'] - price_trend['cleaning_fee_delta']\n",
    "price_trend['cleaning_fee_delta_pct'] = price_trend['cleaning_fee_delta']/price_trend['cleaning_fee_x']*100\n",
    "price_trend['service_fee_delta_pct'] = price_trend['service_fee_delta']/price_trend['service_fee_x']*100\n",
    "price_trend['total_price_delta_pct'] = price_trend['total_price_delta']/price_trend['total_price_x']*100\n",
    "#price_trend['display_price_delta_pct'] = price_trend['display_price_delta']/(price_trend['total_price_x']-price_trend['service_fee_x']-price_trend['cleaning_fee_x'])*100\n",
    "price_trend['display_price_delta_pct'] = price_trend['total_price_delta_pct'] - price_trend['service_fee_delta_pct'] - price_trend['cleaning_fee_delta_pct']\n",
    "data_w_listing_loc = pd.read_csv('post_mort_and_roi_calcs_june19.csv')\n",
    "data_w_listing_loc = data_w_listing_loc[['id','zipcode','City','Metro','CountyName','State']]\n",
    "price_trend['id'] = price_trend['id'].astype('float64')\n",
    "price_trend_wloc = price_trend.merge(data_w_listing_loc, how='inner', on='id')\n",
    "\n",
    "# Get Florida data for example viz\n",
    "florida_price_trends = price_trend_wloc[price_trend_wloc['State'] == 'FL']\n",
    "\n",
    "# Pivot table on zipcode and day of the week\n",
    "florida_price_trends_zip_dow = florida_price_trends.pivot_table(index=['zipcode','day_of_week_x'], values='total_price_delta_pct', aggfunc='mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florida_price_trends_zip_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "#with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "#    counties = json.load(response)\n",
    "\n",
    "with urlopen('https://raw.githubusercontent.com/OpenDataDE/State-zip-code-GeoJSON/master/fl_florida_zip_codes_geo.min.json') as response:\n",
    "    zipcodes = json.load(response)\n",
    "\n",
    "florida_price_trends_zipformap = florida_price_trends.groupby('zipcode').mean('total_price_delta_pct').reset_index()\n",
    "\n",
    "fig = px.choropleth(florida_price_trends_zipformap, \n",
    "                    geojson=zipcodes, \n",
    "                    locations='zipcode', \n",
    "                    color='total_price_delta_pct',\n",
    "                    color_continuous_scale=\"RdYlGn\",\n",
    "                    featureidkey=\"properties.ZCTA5CE10\",\n",
    "                    range_color=(-100,100),\n",
    "                    scope=\"usa\"\n",
    "                    #labels={'Price Change':'Cluster_Category'}\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "filename = \"newsletter_features/florida_price_trends_heatmap_zip_weekly.png\"\n",
    "fig.write_image(filename, engine='kaleido', width=875, height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "#headers = florida_price_trends_zip_dow['zipcode']\n",
    "#vals = florida_price_trends_zip_dow['total_price_delta_pct']\n",
    "#font_color = ['rgb(40,40,40)'] +  [['rgb(255,0,0)' if v < 0 else 'rgb(0,125,0)' for v in vals[k]] for k in range(4)]\n",
    "\n",
    "table_for_dow = go.Table(\n",
    "                 columnwidth= [50]+[50]+[50]+[50]+[50],\n",
    "                 columnorder=[0, 1, 2, 3, 4],\n",
    "                 header = dict(height = 40,\n",
    "                               values = [['<b>Zip Code</b>'], ['<b>Monday</b>'],['<b>Tuesday</b>'],['<b>Wednesday</b>'],['<b>Thursday</b>'],['<b>Friday</b>'],['<b>Saturday</b>'],['<b>Sunday</b>']],\n",
    "                               line = dict(color='rgb(50,50,50)'),\n",
    "                               align = ['left']*5,\n",
    "                               font = dict(color=['rgb(45,45,45)']*4, size=14),\n",
    "                             \n",
    "                              ),\n",
    "                 cells = dict(values=[florida_price_trends_zip_dow['0'],\n",
    "                            florida_price_trends_zip_dow['1'],\n",
    "                            florida_price_trends_zip_dow['2'],\n",
    "                            florida_price_trends_zip_dow['3'],\n",
    "                            florida_price_trends_zip_dow['4'],\n",
    "                            florida_price_trends_zip_dow['5'],\n",
    "                            florida_price_trends_zip_dow['6']],\n",
    "                              line = dict(color='#506784'),\n",
    "                              align = ['left']*5,\n",
    "                              \n",
    "                              #font = dict(family=\"Arial\", size=14, color=font_color),\n",
    "                              format = [None, \",.2f\"],  #add % sign here\n",
    "                              height = 30,\n",
    "                              fill = dict(color='rgb(245,245,245)'))\n",
    "                             )\n",
    "                 \n",
    "\n",
    "layout = go.Layout(width=850, height=650, autosize=False, \n",
    "              title_text='Recent Pricing Trends in Miami (%) (W/W) By Day of Week',\n",
    "                   title_x=0.5, showlegend=False)\n",
    "florda_price_trend_fig = go.Figure(data=[table_for_dow], layout=layout)\n",
    "filename = \"newsletter_features/florida_price_trends_table_dow.png\"\n",
    "florda_price_trend_fig.write_image(filename, engine='kaleido', width=875, height=700)\n",
    "florda_price_trend_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florida_price_trends_zip_dow_linechart = florida_price_trends_zip_dow.reset_index()\n",
    "fig_line_prices_zips = px.line(florida_price_trends_zip_dow_linechart, x=\"day_of_week_x\", y=\"total_price_delta_pct\", color='zipcode',                  labels={\n",
    "                     \"day_of_week_x\": \"Day of Week\",\n",
    "                     \"total_price_delta_pct\": \"Pct Price Change\"\n",
    "                 })\n",
    "fig_line_prices_zips.update_layout(title='Price Changes in Miami W/W Monday-Sunday')\n",
    "fig.update_xaxes(range=[-100, 100])\n",
    "filename = \"newsletter_features/florida_price_trends_lines_zip_weekly.png\"\n",
    "fig.write_image(filename, engine='kaleido', width=875, height=700)\n",
    "fig_line_prices_zips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_trend = pd.read_csv('price_trend_miami_for_table.csv')\n",
    "\n",
    "data_w_listing_loc = pd.read_csv('post_mort_and_roi_calcs_june19.csv')\n",
    "data_w_listing_loc = data_w_listing_loc[['id','lat','lng','zipcode','City','Metro','CountyName','State']]\n",
    "price_trend['id'] = price_trend['id'].astype('float64')\n",
    "price_trend_wloc = price_trend.merge(data_w_listing_loc, how='inner', on='id')\n",
    "\n",
    "price_trend_wloc.to_csv('price_trends_for_map.csv')\n",
    "\n",
    "# Get Florida data for example viz\n",
    "#florida_price_trends = price_trend_wloc[price_trend_wloc['State'] == 'FL']\n",
    "\n",
    "# Pivot table on zipcode and day of the week\n",
    "# Cant do because we dont have data in local csv and now we have new crazy amount of data in local file\n",
    "## -> remove 80% of data in local\n",
    "\"\"\"\n",
    "price_trend_wloc_pvttbl = price_trend_wloc.pivot_table(index=['zipcode','day_of_week_x'], values='total_price_delta_pct', aggfunc='mean')\n",
    "\n",
    "price_trend_wloc_pvttbl.to_csv('price_trend_wloc_pvttbl.csv')\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_trend_wloc.to_csv('price_trends_for_map.csv')\n",
    "\n",
    "\n",
    "price_trend_wloc_pvttbl = price_trend_wloc.pivot_table(index=['zipcode','day_of_week_x'], values='total_price_delta_pct', aggfunc='mean')\n",
    "price_trend_wloc_pvttbl.to_csv('price_trend_wloc_pvttbl.csv')\n",
    "\n",
    "\n",
    "price_trends_wloc_zip_for_table = price_trend_wloc.groupby('zipcode').mean(['display_price_delta_pct','cleaning_fee_delta_pct','service_fee_delta_pct','total_price_delta_pct']).reset_index()\n",
    "price_trends_wloc_zip_for_table.to_csv('price_trends_wloc_zip_for_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for if we want to use Density plots in place of choropleth while working GeoJSON issue for zipcodes\n",
    "\n",
    "import plotly.express as px\n",
    "avg_lat = price_trend_wloc['lat'].mean()\n",
    "avg_lng = price_trend_wloc['lng'].mean()\n",
    "dens_map_price = px.density_mapbox(price_trend_wloc, lat='lat', lon='lng', z='total_price_delta_pct', radius=10,\n",
    "                        center=dict(lat=avg_lat, lon=avg_lng), zoom=3, color_continuous_scale='RdYlGn', range_color=(-100,100),\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    "dens_map_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1105)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1091)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:571)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1633)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:510)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:510)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:510)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mattg\\Desktop\\Hobbies\\airbnb_reports\\price_eda.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdelta\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m builder \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39msql\u001b[39m.\u001b[39mSparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mMyApp\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.extension\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mio/delta.sql.DeltaSparkSessionExtension\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.catalog.spark_catalog\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m spark \u001b[39m=\u001b[39m configure_spark_with_delta_pip(builder)\u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[1;32mc:\\Users\\mattg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    229\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mc:\\Users\\mattg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 392\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\mattg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    144\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[0;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n\u001b[0;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\mattg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[0;32m    210\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Users\\mattg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:329\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_initialize_context\u001b[39m(\u001b[39mself\u001b[39m, jconf):\n\u001b[0;32m    326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[1;32mc:\\Users\\mattg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\mattg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1105)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1091)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:571)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1633)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:510)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:510)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:510)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "        .config(\"spark.sql.extension\",\n",
    "        \"io/delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyDeltaTableError",
     "evalue": "Failed to load checkpoint: Failed to read checkpoint content: Failed to read S3 object content: Request ID: None Body: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>PermanentRedirect</Code><Message>The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.</Message><Endpoint>s3.amazonaws.com</Endpoint><Bucket>airbnb-scraper-bucket-0-1-1</Bucket><RequestId>Y7AHWRZKEJ0GB5FG</RequestId><HostId>J3sjxqQ34il/SWVEXK33NosYwCMDGaiuU7NmMemc9I4K6pHZdFw57LAkSqXTBOWv7FiYuJfboDU=</HostId></Error>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPyDeltaTableError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mattg\\Desktop\\Hobbies\\airbnb_reports\\price_eda.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeltalake\u001b[39;00m \u001b[39mimport\u001b[39;00m DeltaTable\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#people = spark.read.format('delta').load('s3://airbnb-scraper-bucket-0-1-1/data/beta_data_tables/occupancy_lag_metrics/')\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mattg/Desktop/Hobbies/airbnb_reports/price_eda.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dt \u001b[39m=\u001b[39m DeltaTable(\u001b[39m'\u001b[39;49m\u001b[39ms3://airbnb-scraper-bucket-0-1-1/data/beta_data_tables/occupancy_lag_metrics/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\mattg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\deltalake\\table.py:90\u001b[0m, in \u001b[0;36mDeltaTable.__init__\u001b[1;34m(self, table_uri, version, storage_options)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     77\u001b[0m     table_uri: \u001b[39mstr\u001b[39m,\n\u001b[0;32m     78\u001b[0m     version: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     79\u001b[0m     storage_options: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m ):\n\u001b[0;32m     81\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m    Create the Delta Table from a path with an optional version.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    Multiple StorageBackends are currently supported: AWS S3, Azure Data Lake Storage Gen2, Google Cloud Storage (GCS) and local URI.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m    :param storage_options: a dictionary of the options to use for the storage backend\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_table \u001b[39m=\u001b[39m RawDeltaTable(\n\u001b[0;32m     91\u001b[0m         table_uri, version\u001b[39m=\u001b[39;49mversion, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m     92\u001b[0m     )\n\u001b[0;32m     93\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata \u001b[39m=\u001b[39m Metadata(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_table)\n",
      "\u001b[1;31mPyDeltaTableError\u001b[0m: Failed to load checkpoint: Failed to read checkpoint content: Failed to read S3 object content: Request ID: None Body: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>PermanentRedirect</Code><Message>The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.</Message><Endpoint>s3.amazonaws.com</Endpoint><Bucket>airbnb-scraper-bucket-0-1-1</Bucket><RequestId>Y7AHWRZKEJ0GB5FG</RequestId><HostId>J3sjxqQ34il/SWVEXK33NosYwCMDGaiuU7NmMemc9I4K6pHZdFw57LAkSqXTBOWv7FiYuJfboDU=</HostId></Error>"
     ]
    }
   ],
   "source": [
    "from deltalake import DeltaTable\n",
    "\n",
    "#people = spark.read.format('delta').load('s3://airbnb-scraper-bucket-0-1-1/data/beta_data_tables/occupancy_lag_metrics/')\n",
    "dt = DeltaTable('s3://airbnb-scraper-bucket-0-1-1/data/beta_data_tables/occupancy_lag_metrics/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8cda3f1edda61d067a8dccfe73833b0b57e2a1d4ac0e36527d0cd620f4d2a66c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
